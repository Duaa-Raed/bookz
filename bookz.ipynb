{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNmo9sjsazNEVzoW3+JxRxy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Duaa-Raed/bookz/blob/main/bookz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ========== IMPORTS (Clean + Organized) ==========\n",
        "\n",
        "# Basic\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Colab utilities\n",
        "from google.colab import files\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Embeddings & Models\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Vector Search\n",
        "import faiss\n",
        "\n",
        "# LangChain Tools (use only if needed)\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS as LC_FAISS\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# HuggingFace models\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "# Gemini\n",
        "import google.generativeai as genai\n"
      ],
      "metadata": {
        "id": "FpIZi9fVhMWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uNE5-zTWyNY"
      },
      "outputs": [],
      "source": [
        "\n",
        "#  Load the first file\n",
        "print(\" Upload the first file (e.g., books.csv):\")\n",
        "uploaded1 = files.upload()\n",
        "df = pd.read_csv(list(uploaded1.keys())[0])\n",
        "print(\" File loaded:\", list(uploaded1.keys())[0])\n",
        "print(df.columns)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first 5 rows\n",
        "print(\" Data Overview:\")\n",
        "print(df.head())\n",
        "\n",
        "# Quick info about columns\n",
        "print(\"\\n Column Information:\")\n",
        "print(df.info())\n",
        "\n",
        "# Numerical statistics\n",
        "print(\"\\n General Statistics:\")\n",
        "print(df.describe())\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n Missing Values:\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "rXo9dj6Fre4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Exploratory Data Analysis (EDA)\n",
        "\n",
        "# Configure font settings for Arabic display (if needed)\n",
        "plt.rcParams['font.family'] = 'Arial'\n",
        "\n",
        "# 1ï¸ Most Productive Authors\n",
        "top_authors = df['Author'].value_counts().head(10)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=top_authors.values, y=top_authors.index, palette=\"viridis\")\n",
        "plt.title(\" Most Productive Authors\")\n",
        "plt.xlabel(\"Number of Books\")\n",
        "plt.ylabel(\"Author\")\n",
        "plt.show()\n",
        "\n",
        "# 2ï¸ Most Common Categories\n",
        "top_categories = df['Category'].value_counts().head(10)\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.barplot(x=top_categories.values, y=top_categories.index, palette=\"magma\")\n",
        "plt.title(\" Most Common Categories\")\n",
        "plt.xlabel(\"Number of Books\")\n",
        "plt.ylabel(\"Category\")\n",
        "plt.show()\n",
        "\n",
        "# 3ï¸ Price and Pages Analysis\n",
        "print(\" Price Statistics:\")\n",
        "print(df['Price'].describe())\n",
        "\n",
        "print(\"\\n Pages Statistics:\")\n",
        "print(df['Pages'].describe())\n",
        "\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.subplot(1,2,1)\n",
        "sns.histplot(df['Price'], bins=40, kde=True, color='orange')\n",
        "plt.title(\"Price Distribution\")\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "sns.histplot(df['Pages'], bins=40, kde=True, color='blue')\n",
        "plt.title(\"Pages Distribution\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "# 4ï¸ Relationship between Price and Pages\n",
        "plt.figure(figsize=(7,6))\n",
        "sns.scatterplot(data=df, x=\"Pages\", y=\"Price\", alpha=0.6)\n",
        "plt.title(\" Relationship between Price and Pages\")\n",
        "plt.xlabel(\"Number of Pages\")\n",
        "plt.ylabel(\"Price (Dinar)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-V6jiiZb_t68"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Delete rows where pages or prices are zero\n",
        "df_clean = df[(df['Pages'] > 0) & (df['Price'] > 0)]\n",
        "\n",
        "print(\"Number of books after initial cleaning:\", len(df_clean))\n",
        "\n",
        "\n",
        "df_clean = df_clean[(df_clean['Price'] < 500) & (df_clean['Pages'] < 2000)]\n",
        "\n",
        "print(\"After removing outliers:\", len(df_clean))\n",
        "\n",
        "print(\"Prices after cleaning:\")\n",
        "print(df_clean['Price'].describe())\n",
        "\n",
        "print(\"\\nPages after cleaning:\")\n",
        "print(df_clean['Pages'].describe())"
      ],
      "metadata": {
        "id": "CVO9f2nfCetg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean texts from null values\n",
        "df.fillna(\"\", inplace=True)\n",
        "\n",
        "# Create a unified text column\n",
        "df[\"text\"] = (\n",
        "    \" Title: \" + df[\"Title\"].astype(str) +\n",
        "    \" Author: \" + df[\"Author\"].astype(str) +\n",
        "    \" Category: \" + df[\"Category\"].astype(str) +\n",
        "    \" Description: \" + df[\"Description\"].astype(str)\n",
        ")\n",
        "\n",
        "print(\"Texts prepared:\", len(df))\n",
        "print(df[\"text\"].head(3))"
      ],
      "metadata": {
        "id": "dU1qd-PdDeYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Load the data file (select the file from your device)\n",
        "print(\" Upload the books file (e.g., jamalon dataset.csv):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the file\n",
        "df1 = pd.read_csv(list(uploaded.keys())[0])\n",
        "print(\" File loaded:\", list(uploaded.keys())[0])\n",
        "print(df1.columns)\n",
        "\n",
        "#  Create a unified text column for title and description\n",
        "if \"Title\" in df1.columns and \"Description\" in df1.columns:\n",
        "    df1[\"text\"] = df1[\"Title\"].fillna('') + \" - \" + df1[\"Description\"].fillna('')\n",
        "    print(\" The 'text' column was created successfully!\")\n",
        "else:\n",
        "    print(\" Make sure you have 'Title' and 'Description' columns in your file.\")\n"
      ],
      "metadata": {
        "id": "-CUbBSD1Gkhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the model\n",
        "model = SentenceTransformer(\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\")\n",
        "\n",
        "# Create the embeddings\n",
        "embeddings = model.encode(df1[\"text\"].tolist(), show_progress_bar=True)\n",
        "\n",
        "# Convert to float32 for FAISS\n",
        "embeddings = np.array(embeddings).astype(\"float32\")\n",
        "\n",
        "# Build the search index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "index.add(embeddings)\n",
        "\n",
        "print(f\"Knowledge base created successfully! Number of books: {len(embeddings)}\")"
      ],
      "metadata": {
        "id": "-i6VOQZLD9NS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "# Add the embeddings to the index\n",
        "index.add(embeddings)\n",
        "print(\" Search database built:\", index.ntotal)"
      ],
      "metadata": {
        "id": "our6_jGnEVxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask(query, k=3):\n",
        "    query_emb = model.encode([query]).astype(\"float32\")\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "    results = df.iloc[indices[0]]\n",
        "    for i, row in results.iterrows():\n",
        "        print(f\" {row['Title']} â€” {row['Author']}\")\n",
        "        print(f\" {row['Description'][:250]}...\")\n",
        "        print(\"â€”\" * 60)\n"
      ],
      "metadata": {
        "id": "VBh8hFRON60h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We use a lightweight model that supports Arabic and English\n",
        "# Note: Ensure you have the necessary environment setup (GPU, bitsandbytes, accelerate) to run this model locally.\n",
        "generator = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
        "\n",
        "def rag_answer(query, k=3):\n",
        "    # Search for the nearest books\n",
        "    query_emb = model.encode([query]).astype(\"float32\")\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "    results = df1.iloc[indices[0]]\n",
        "\n",
        "    # Aggregate texts from the books\n",
        "    context = \"\\n\\n\".join(\n",
        "        [f\" {r['Title']} â€” {r['Author']}\\n{r['Description']}\" for _, r in results.iterrows()]\n",
        "    )\n",
        "\n",
        "    # Input prompt for the model\n",
        "    prompt = f\"\"\"\n",
        "    User Question: {query}\n",
        "\n",
        "    Texts retrieved from the books (Context):\n",
        "    {context}\n",
        "\n",
        "     Answer concisely and naturally, relying ONLY on the texts provided above.\n",
        "    \"\"\"\n",
        "\n",
        "    # Generate the answer\n",
        "    answer = generator(prompt, max_new_tokens=200)[0]['generated_text']\n",
        "    print(\" Answer:\\n\", answer)"
      ],
      "metadata": {
        "id": "b6N0H1VaWM65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rag_answer(question, retriever, model, tokenizer):\n",
        "    #  Step 1: Retrieve the chunks (sources) most relevant to the question\n",
        "    docs = retriever.get_relevant_documents(question)\n",
        "    context = \"\\n\".join([d.page_content for d in docs])\n",
        "\n",
        "    #  Step 2: Prepare the full prompt for the LLM (Question + Context)\n",
        "    prompt = f\"Question: {question}\\n\\nAvailable Information:\\n{context}\\n\\nDetailed Answer:\"\n",
        "\n",
        "    #  Step 3: Let the model generate the answer based on the context\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=300)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    return answer"
      ],
      "metadata": {
        "id": "oZVY_-JEYj7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the books file\n",
        "print(\" Upload the books file (e.g., jamalon dataset.csv):\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the file\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "print(\" File loaded:\", list(uploaded.keys())[0])\n",
        "print(df.columns)\n",
        "\n",
        "# Create a unified text column\n",
        "if \"Title\" in df.columns and \"Description\" in df.columns:\n",
        "    df[\"text\"] = df[\"Title\"].fillna('') + \" - \" + df[\"Description\"].fillna('')\n",
        "    print(\" The 'text' column was created successfully!\")\n",
        "else:\n",
        "    print(\" Make sure the 'Title' and 'Description' columns exist in the file.\")"
      ],
      "metadata": {
        "id": "yobOlQjUezBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the embedding model\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Convert DataFrame rows into LangChain Document objects\n",
        "documents = [Document(page_content=row[\"text\"]) for _, row in df.iterrows()]\n",
        "\n",
        "# Create the FAISS vector store from the documents and embeddings\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "# Create a retriever object for similarity search (top 3 results)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "print(\" Vector search base built successfully!\")"
      ],
      "metadata": {
        "id": "VI7Q6stzYyk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Load the books file\n",
        "print(\" Upload the books file (e.g., jamalon dataset.csv):\")\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "print(\" File loaded:\", list(uploaded.keys())[0])\n",
        "\n",
        "# Create a unified text column\n",
        "df.fillna(\"\", inplace=True)\n",
        "df[\"text\"] = (\n",
        "    \" Title: \" + df[\"Title\"].astype(str) +\n",
        "    \" Author: \" + df[\"Author\"].astype(str) +\n",
        "    \" Category: \" + df[\"Category\"].astype(str) +\n",
        "    \" Description: \" + df[\"Description\"].astype(str)\n",
        ")\n",
        "\n",
        "#  Create the vector documents (Embeddings)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "documents = [Document(page_content=row[\"text\"]) for _, row in df.iterrows()]\n",
        "vectorstore = FAISS.from_documents(documents, embeddings)\n",
        "\n",
        "# Create the retriever (search tool)\n",
        "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "print(\" Vector search base built successfully!\")"
      ],
      "metadata": {
        "id": "Durh3qGzmhfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#  Load the generation model\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n"
      ],
      "metadata": {
        "id": "3ShFgnuGjE0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_books_bot(query):\n",
        "    #  Retrieve results from the database\n",
        "    results = retriever.invoke(query)\n",
        "\n",
        "    # Filter results to include only texts containing the requested book name\n",
        "    filtered = [doc for doc in results if any(word in doc.page_content for word in query.split())]\n",
        "\n",
        "    if not filtered:\n",
        "        print(\" Sorry, no information is available for this book.\")\n",
        "        return\n",
        "\n",
        "    # Merge the found texts\n",
        "    context = \"\\n\\n\".join([doc.page_content for doc in filtered])\n",
        "\n",
        "    #  Prepare the prompt\n",
        "    prompt = f\"\"\"You are an intelligent assistant specialized in books and novels.\n",
        "Rely only on the information below to answer briefly and clearly without reiterating it.\n",
        "\n",
        "Knowledge:\n",
        "{context}\n",
        "\n",
        "User question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    #  Generate the answer\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(**inputs, max_new_tokens=180)\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Clean the output\n",
        "    if \"Answer:\" in answer:\n",
        "        answer = answer.split(\"Answer:\")[-1].strip()\n",
        "\n",
        "    # Check quality\n",
        "    if not answer or len(answer) < 15:\n",
        "        print(\" Sorry, no information is available for this book.\")\n",
        "    else:\n",
        "        print(\"ðŸ¤–\", answer)"
      ],
      "metadata": {
        "id": "Md9Y167Yjne_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "#  Final Code (Cleaned and Simplified Version)\n",
        "# ==========================================\n",
        "\n",
        "# 1ï¸ Gemini API Setup\n",
        "print(\" Get a free API Key from: https://makersuite.google.com/app/apikey\")\n",
        "api_key = input(\"Enter API Key: \").strip()\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "#  Latest and most compatible model\n",
        "gemini_model = genai.GenerativeModel('gemini-2.5-flash')\n",
        "print(\" Gemini activated!\")\n",
        "\n",
        "# 2ï¸ Load Data\n",
        "print(\"\\n Upload the books file:\")\n",
        "# Make sure to re-upload the file or that it exists in the runtime environment\n",
        "uploaded = files.upload()\n",
        "df = pd.read_csv(list(uploaded.keys())[0])\n",
        "\n",
        "df.fillna(\"\", inplace=True)\n",
        "df[\"text\"] = (\n",
        "    df[\"Title\"].astype(str) + \" | \" +\n",
        "    df[\"Author\"].astype(str) + \" | \" +\n",
        "    df[\"Description\"].astype(str)\n",
        ")\n",
        "\n",
        "# 3ï¸ Create Embeddings\n",
        "print(\"ðŸ” Creating search base...\")\n",
        "\n",
        "# Clean GPU memory\n",
        "if 'embeddings' in locals() or 'embeddings' in globals():\n",
        "    del embeddings\n",
        "if 'model' in locals() or 'model' in globals():\n",
        "    del model\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "print(\" GPU memory cleaned.\")\n",
        "\n",
        "# Use the correct model all-MiniLM-L6-v2\n",
        "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "\n",
        "# Keep batch size small\n",
        "BATCH_SIZE = 16\n",
        "embeddings = model.encode(df[\"text\"].tolist(), show_progress_bar=True, batch_size=BATCH_SIZE)\n",
        "\n",
        "embeddings_array = np.array(embeddings).astype('float32')\n",
        "index = faiss.IndexFlatL2(embeddings_array.shape[1])\n",
        "index.add(embeddings_array)\n",
        "print(f\" Ready! ({len(df)} books)\")\n",
        "\n",
        "# 4ï¸ Search function + Gemini (Final output modification)\n",
        "def ask_gemini(query, k=3):\n",
        "    \"\"\"Smart search with answers from Gemini\"\"\"\n",
        "\n",
        "    # Search in the database and aggregate context (unchanged)\n",
        "    query_emb = model.encode([query]).astype('float32')\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "\n",
        "    books_info = []\n",
        "    for idx in indices[0]:\n",
        "        book = df.iloc[idx]\n",
        "        books_info.append(f\"\"\"\n",
        " {book['Title']}\n",
        " Author: {book['Author']}\n",
        " Description: {book['Description'][:300]}\n",
        "\"\"\")\n",
        "    context = \"\\n\\n\".join(books_info)\n",
        "\n",
        "    # Prepare Prompt for Gemini (unchanged)\n",
        "    prompt = f\"\"\"You are an intelligent library assistant. Use the following information to answer:\n",
        "{context}\n",
        "User question: {query}\n",
        "Provide a helpful and concise answer (3-4 sentences only), and mention relevant book titles.\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Get answer from Gemini\n",
        "        response = gemini_model.generate_content(prompt)\n",
        "\n",
        "        #  Modified parts for clean printing:\n",
        "        print(\"\\n**Answer:**\")\n",
        "        print(response.text)\n",
        "\n",
        "        #  Suggested Books\n",
        "        print(\"\\n--- Suggested Books ---\")\n",
        "\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            book = df.iloc[idx]\n",
        "            print(f\"{i+1}. {book['Title']} - {book['Author']}\")\n",
        "        # --------------------------------------------------\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error with Gemini: {e}\")\n",
        "        print(\" Displaying results directly (generation failed):\")\n",
        "        for i, idx in enumerate(indices[0]):\n",
        "            book = df.iloc[idx]\n",
        "            print(f\"\\n{i+1}.  {book['Title']}\")\n",
        "            print(f\"    {book['Author']}\")\n",
        "            print(f\"    {book['Description'][:200]}...\")\n",
        "\n",
        "# ==========================================\n",
        "#  Experiment (removing initial examples)\n",
        "# ==========================================\n",
        "print(\"\\n\\n System ready!\")\n",
        "\n",
        "# Interactive mode\n",
        "print(\"\\n\\n Ask me any question (or 'exit' to quit):\")\n",
        "while True:\n",
        "    #  Remove '\\n' from input line to avoid duplicate empty line\n",
        "    q = input(\" Your question: \").strip()\n",
        "    if q.lower() in ['exit', 'quit']:\n",
        "        break\n",
        "    if q:\n",
        "        ask_gemini(q)"
      ],
      "metadata": {
        "id": "rnIEKkTUPywa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}